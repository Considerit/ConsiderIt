#description{ :class => "#{hide_initially ? 'hiding' : ''}"}

  .description_wrapper

    #col1
      .user
        = image_tag current_theme_image_path "profile.png"

    #col2

      .init_text        
        %h1
          How would you review this paper?
        %h2.author
          Travis Kriplean
      
      .full_text
        
        %h3 Let's have a grounded discussion about evaluating social systems.

        %p
          Despite 
          %a{:href => 'http://dubfuture.blogspot.com/2009/11/i-give-up-on-chiuist.html', :target=>"_blank"}
            many conversations 
          in the CHI community about the role of system/design contributions, it is unclear whether much headway is being made. Instead of having yet another abstract discussion about it, let's review a social systems contribution together, as a community. This will help us better establish where we share common criteria and where we diverge. 

          %span.outset
            %span.triangle
              &#x25C0;
            This continues the conversation Michael Bernstein started about
            = link_to 'evaluating social systems contributions', 'http://people.csail.mit.edu/msbernst/papers/trouble-altchi2011.pdf'

        %p
          Here I have posted my CHI submission and the reviews. It reports on two years of work. The reviews highlight the tension in our community between creating new possible futures and "objective" validation of those systems, with two generally positive reviews (R1 & R2), a very negative review (R3), and a neutral meta review.

        %p          
          What are the appropriate criteria by which to evaluate this paper, and more broadly, this style of research? Read the paper and think about what aspects of the research stimulated you. Then compare it with what the reviewers focused on. Are these reviews focusing on the aspects of the contributions that you think are important? If so, why? If not, what are the appropriate criteria?

        %p.personal_thought
          %span.hide
            %strong
              IMHO: CHI reviewing dissuades new social systems & inventors
            %br
            The mindset exposed through the reviews is disappointing and discouraging. In what is ostensibly a field spanning innovation and science in seeking to create a better future, why is there a lack of awareness that there can be valuable academic work that does not objectively prove a set of claims? The style of research I practice creates new design-oriented framings of challenging problems, creates novel systems grounded in the frame, deploys them in real situations, and examines what happened in order to understand if the approach has some basic traction outside of the Ivory Tower. When reviewers demand generalizable, "objective" findings alongside these other contributions, the potential of the field is undercut because it does not recognize the practical need for a division of intellectual labor in making progress. This narrowmindedness drives away inventors, something that is very salient to me right now as I graduate and decide whether academia is the right place to try to continue creating new systems for public discussion.

            %span.outset.status
              %span.triangle
                &#x25C0;          
              %span.quote
                &ldquo;I cannot help wondering what the status of scientific research in the CHI domain is if this paper comes to be accepted as a full research paper.&rdquo;
              %span.signature
                &mdash; R3     
                  
          %span.outset.personal
            %span.triangle
              &#x25C0;          
            %a.prompt
              My personal opinion.
          
        :javascript
          $j('.personal_thought a.prompt').click(function(){
            var me = $j(this).parent();
            $j('.personal_thought .hide').slideDown('slow', function(){
              me.fadeOut();
            });
          });

        %h3
          Participate using this new platform for reviewing papers.

        %p
          Conveniently, my research is about creating new systems for community deliberation. I have adapted two of my systems, one of which is the subject of the posted paper, to support this discussion and what I hope you find to be a more rewarding reviewing experience.

        %p
          Please spread the word!!!




 


    #col3    
      .paper_link.add_corner.shadow_centered
      
        %a{:href => "http://www.cs.washington.edu/homes/travis/papers/reflect_chi_submitted.pdf", :target => '_blank'}
          Read the paper
                  
        -# %object{:data => "http://www.cs.washington.edu/homes/travis/papers/reflect_chi_full.pdf", :type => "application/pdf", :width => "850px", :height => "750px"}
           
              

      .additional
        .prompt
          .item.fl
            Reviewer 1
          %a.hidden.fr
            show
          .clear
        .full.hide

          %h3
            Your Assessment of this Paper's Contribution to HCI

          %p
            Improvement of  the quality of online discussion through software
            functionalities and features which are directly reflected in the tool
            interface.
          %p
            The paper presents a complete piece of research which covers foundations,
            design, implementation, deployments and their evaluation.
            References are rich and well rooted in the communication science, CHI and
            CSCW literature.
          %p
            It is relevant for all the fields where online discussions do occur, in
            particular in the public and non profit sector for gathering feedbacks
            and ideas from engaged citizens.

          %h3
            Overall Rating

          %p
            4.5 . . . Between possibly accept and strong accept 

          %h3
            Review

          %p
            The paper presents Reflect, a tool for improving the quality of online
            discussion through restatement, i.e., by allowing participants to attach
            to each post in a discussion thread a bulleted list. Each item of the
            list (no longer than 140 characters) is an “an act of listening” which
            helps “distilling meanings”. 

          %p
            The paper is very well written and organized. It presents the foundations
            behind the work, by discussing listening as a communication discipline.
            It then presents the design rational, with a  comparison with similar
            tool, and gives some hints on the implementation: Reflect is actually a
            plug-in which can be added to existing discussion tools; this plug-in is
            said to be presently available for Wordpress, Drupal, MediaWiki and Slash
            platforms (but on the Reflect website I don’t see Drupal). 

          %p
            The results of the tool deployment in different contexts are then
            presented: three are real-life contexts (Slashdot, Wikimedia and King
            County Community Forum), while the third one is a controlled social
            experiment. The results, well presented and commented (but for the remark
            2 to the authors), are said to be positive especially in the cases of
            Slashdot and Wikimedia, not significant in the case of the King County
            Community Forum (“As a new arm of a relatively new, regional initiative,
            the website does not attract nearly as many comments as a site like
            Slashdot”), negative in the third one. 

          %h3
            Expertise
          
          %p
            4    (Expert )
            
          %h3
            Areas for Improvement

          %ol
            %li
              you often speak of “interfaces” (e.g., “REFLECT: LISTENING THROUGH
              RESTATEMENT In this section, we present a novel interface….”). I don’t
              believe it is actually matter of interface, it is matter of
              functionalities of the online discussion tool and  of its features, which
              are of course  reflects (sorry for the pun) in the interface. (I use here
              “tool” and “features”  according to the hierarchy proposed by Wenger et
              al., in the CEFRIO Book Chapter v 5.2. (2005) “Technology for
              communities“ by  Etienne Wenger, Nancy White, John D. Smith, and Kim
              Rowe). That is is actually mater of features come out in your words in
              the last sentence, page 8.

            %li
              please provide, always when possible, the URL for the real cases and
              experiments. I could only check the evaluation of the deployments in the
              case of the King County Community Forum, where I indeed see a very
              limited use of Reflect facilities. This could support arguments that,
              when ordinary people are involved, problems in the use of the tool arise,
              due to its (a bit) normative nature. The positive cases, Slashdot and
              Wikimedia, come from communities of much more skilled users.

            %li
              In the demos and cases that I’ve been able to find online, I never saw
              the icons, shown to the top left side in Fig. 1, to classify bulleted
              items as “elegantly distill meaning? Uncover a good point? etc.

            %li
              in its first occurrence, explain the A/B  acronym

            %li
              in the section Design rational, point 3., I suggest to swap the first
              and the second point: accountability become clearer after readability is
              presented

            %li
              in the very last sentence of the Conclusion there is  a double “in” (“a new point in in this design space”)

      .additional
        .prompt
          .item.fl
            Reviewer 2
          %a.hidden.fr
            show
          .clear
        .full.hide

          %h3
            Your Assessment of this Paper's Contribution to HCI

          %p
            This describes the design and evaluation of a prototype web technology
            called Reflect. Reflect provides an interface which allows users to
            restate and summarize comments made by other users on some web resource
            that features user generate content (in the form of comments). Reflect's
            goal is to act as support for "listening" - i.e. demonstrating an
            understanding of what other people are saying through "restatement", to
            create common ground and support a more constructive discussion. This
            objective originates from the observation that "public discourse on the
            web is often inflammatory and hyperbolic", and that this does not have to
            be the case in spite of certain inherent limitations of the web as a
            medium for communication (e.g. asynchronous communication).

          %h3
            Overall Rating

          %p 
            4.0 - Possibly Accept: I would argue for accepting this paper


          %h3
            Review

          %p
            Overall I think this is a good paper that presents a substantial body of
            work. I would argue for its acceptance although there I think the
            discussion of the experimental deployment of the prototype could be
            enriched to better focus the objectives and the potential usefulness of
            this research. The basic premise (that “constructive web discussion” is
            elusive) and the following hypothesis (“lack of attention paid to
            supporting listening”) are clear to me. I don’t want to take issue with
            the linguistic theories the authors refer to – the relationship between
            the concepts they use and the design choices they make is clearly
            explained. What I think would help clarify the paper even further is a
            more explicit description of what communication “scenarios” they are
            targeting their technology and why. Promoting understanding on the
            assumption that the primary goal of all types of web sites that contain
            user generated content is “communication” is a very broad goal. There are
            web resources where the goal is to promote discussion (and that therefore
            will want to summarize and “capture” the best value out of any discussion
            that takes place) and others where the presence of discussion is
            incidental (most news sites where posted articles and related comments
            have a very short shelf-life).  There are web resources that belong to
            very specific communities of interest or practice (who may not feel the
            need for or respond well to an additional layer of moderation) and others
            which are open ended and open minded. There is plenty of research on the
            classification of web-sites to draw on, and the three experimental
            contexts described in the paper seems to be different enough to provide
            some discussion of this, so in a way it seems a bit of a missed
            opportunity that the analysis of the results remains essentially
            methodological and “partial” for each of the three (i.e. one presents a
            quantitative breakdown of the “discretionary” use of Reflect, one
            presents the results of a survey aimed at assessing whether Reflect
            “improves” a discussion and one presents qualitative interviews with
            facilitators). 

          %h3
            Expertise

          %p
            2    (Passing Knowledge)

          %h3
            Areas for Improvement

          %p
            On a minor note – the text description of Table ** is truncated.

      .additional
        .prompt
          .item.fl
            Reviewer 3
          %a.hidden.fr
            show
          .clear

        .full.hide

          %h3
            Your Assessment of this Paper's Contribution to HCI

          %p
            The paper contributes in making some dynamics of face-to-face
            communication (namely listening and "grounding", after Clark) available
            also through the distributed and asynchronous interaction that current
            web-based (and text-intensive) social networks and media provide users
            with just in terms of the typical posting/commenting exchange.

          %h3
            Overall Rating
          
          %p
            2.0 - Possibly Reject: The submission is weak and probably shouldn't be accepted, but there is some chance it should get in.

          %h3
            Review

          %p
            As it is manifest from the overall rating I gave, I did not like the
            paper much. This is for two main reasons, one subjective (as it is any
            rating, after all), and one more objective, which obviously mattered more
            than the former. 

          %p
            Thus, on the one hand, I do not feel to buy the general tenet about the
            application that the authors propose: namely that Reflect should
            (contribute to) overcome the current shortcomings of web-mediated
            collaborative communication. In fact, I believe that such an application,
            which asks users to reformulate postings and comments and classify these
            restatements, could indeed _increase_ the "noise" and the opportunities
            of either misunderstanding or flaming that necessarily flaw much of this
            kind of interaction, instead of making communication (or sense-making)
            easier, deeper, or both, as it aims to reach such a far-reaching goal by
            nothing more articulated than a (written!) mechanism to demonstrate
            evidence of listening (metaphorically speaking) through restatement. 
          %p
            On the other hand, the second, and more serious, reason for my low rating
            is that, although what I have just expressed is just an opinion, the
            paper itself does not really go beyond the mere (although respectable)
            opinion of the authors. 
            I know that this could sound odd at a first reading, as the paper
            includes a sort of validation of the proposal in three different and
            complementary domains, and also it does a great job in packaging a
            _seemingly_ sound experimentation/discussion for each of these case
            studies. The point is that, at least from the methodological point of
            view (which I care most), the three case studies do not really go beyond
            "case study reporting".
            I am not denying that this kind of report could be of some interest for
            someone in CHI (hence my rating of 2.0, instead of a perhaps more
            appropriate 1.5) but I cannot help wondering what the status of
            scientific research in the CHI domain is if this paper comes to be
            accepted as a full research paper.
          %p
            I will try to make my point more clear. The promising hypotheses outlined
            in the Introduction (a section that I really appreciated and that made me
            hope for more as good as it) are left completely unaddressed (both
            formally and informally) by the authors: at the end of the paper, I was
            still wondering what research question the paper was _really_ addressing,
            beyond the very general one about how to "support listening" on the web. 
          %p
            More precisely, in context 1 the authors report relative and absolute
            percentages of users that restated the posted comments on slashdots, but
            the research questions they progressively address can be related to these
            figures only very loosely (e.g., they say that the fact that almost 90%
            of the bullet contributors did not contribute comments is somehow
            correlated to the effect that Reflect might have in encouraging
            participation from "lurkers"; yet, this correlation is very far away from
            being seriously addressed in the paper, leave alone proved). Again, the
            authors have undertaken a content analysis whose methodology I do not
            question, but whose conclusions are just plain wrong. As far as I know (I
            have personally applied the Krippendorff's alpha in some studies of mine)
            the conventional interpretation of this score of inter-rater reliability
            requires alphas to be bigger than 0.8 (even wikipedia says something
            similar). The highest score (for just one code) that the authors get is
            0.7, while the other codes rank well below this threshold... so, what are
            they discussing about? Although authors considered to have reached "high
            reliability" (sic!) for just one code (in all fairness), in fact they
            could not code comments/restatements reliably at all, and this -to my
            eyes- invalidates the whole discussion (alas) (i.e., not necessarily the
            fact that some of their points are still plausible, but their scientific
            soundness). 
          %p
            In context 2 the authors extensively quote the enthusiastic opinion of
            one user (although such an opinion is quite influential, that user
            represents just her/his own opinion), and then analyse an ex-post open
            question questionnaire that had been administered to four (4!) users. I
            thought to have taken this wrong when I read that "several participants
            independently noted that...". The authors here are reporting a sort of
            "focus-group like" validation/evaluation that still, from the
            methodological point of view, perplexes me. 
          %p
            In context 3 we are even more clearly confronted with less than a
            work-in-progress analysis. Here the authors (as honestly as before re
            context 1) come to sharing with the readers their perplexity on the fact
            that their tool ranked worse than not having it along some qualitative
            dimensions of exit evaluation. Here I suggest them to perform a _non_
            parametric test (like the Mann-Whitney U) on the null hypothesis that the
            observed differences in the median (and not the means, since the scale is
            ordinal, not interval!) are real (or substantial), and then rejoice at
            discovering that these differences are probably not (i.e., that there is
            no evidence there is a difference, neither for the better nor for the
            worse). The analitycal method employed to back the discussion about
            context 3, therefore, is way worse than the one undertaken for the other
            two cases and it is in my opinion well below the minimum quality
            threshold for a CHI submission. 
          %p
            Thus, all things considered, I agree that the authors have gathered a lot
            of significant (although highly qualitative and necessarily subjective)
            user data in their hands. That notwithstanding, I suggest them to
            consider to focus on just one context (leaving, e.g., context 3 for a
            more meditated and future study), discuss it in the light of a simple but
            still methodologically sound approach, and try to reformulate maybe even
            just one clear hypothesis (or research question) and validate it by the
            book.

          %h3
            Expertise

          %p
            3    (Knowledgeable)
            
          %h3
            Areas for Improvement

          %p
            I really do not like the authors' choice to call their potential users
            "listeners" (and their system a "listening interface"). This led me
            astray for almost 5 columns of text, before I could get what the paper
            was about (sort of). I see that their tool has already been deployed in
            the large (I also tried it on my drupal instance) and that their research
            has already been published in some scientific venues: thus, I toss my
            opinion in just to raise the need for the authors to anticipate the point
            they felt the need to make at the end of the Introduction. They should
            explain much earlier that "listening" here refers to providing writers
            with written clues of active comprehension and commitment, and that a
            "listener" in their context is just one that summarizes/restates a
            previously created passage in a written form. In short: OK with the Clark
            theory on grounding, just let it be more clear a little earlier.  
          %p
            The related work section (which is good) is weighed down by some
            expressions that look too vague or wary for a research paper (e.g.,
            "Reflect might be said... Reflect may in fact be the first... systems
            that are designed to support grounding tend to be...). I'd rather see
            those passages reformulated in terms of "Reflect IS, to our knowledge,
            the first interface..." and the like.
          %p
            I must have missed the point on who evaluated the quality of the comments
            mentioned in context 3 (more precisely, subsection "behavioral results")
            and how she undertook such an assessment. 
          %p
            The footnote in Section "Deployment in three contexts and results" is not
            really necessary. I even guessed the authors were actually still flaming
            against the user 375juvenile...

      .additional
        .prompt
          .item.fl
            Meta Review
          %a.hidden.fr
            show
          .clear

        .full.hide

          %h3
            Your Assessment of this Paper's Contribution to HCI

          %p
            The paper presents a technology allowing people involved in an online 
            discussion to manifest their "listening" through a special kind of
            comment that allows a short summary of what has been discussed.  The
            functionality has been tested in three different conditions; the paper
            presents the related results.

          %h3
            Overall Rating

          %p 
            3.0 - Neutral: Overall I would not argue for accepting or rejecting this paper.

          %h3
            Expertise
          
          %p
            3    (Knowledgeable)

          %h3
            The Meta-Review

          %paper
            The paper proposes a functionality to summarize annotations so as an
            actor can play as active listener in an on-line discussion. The
            contribution is well positioned in the literature although the paper does
            not contain an explicit section on related work.
            The paper received controversial reviews, each focusing on different
            aspects: one review sees Reflect from the standpoint of the public/non
            profit sector where its features could be useful; the second review
            appreciates the basic idea but questions its applicability/usefulness in
            a undifferentiated context and requires a deeper consideration of this
            aspect in presenting the results gathered from the three deployment
            contexts; the third review, the most negative, questions the basic idea
            to some extent and more importantly, the soundness or meaningfulness of
            the validation results.

          %p
            All reviews contain valid argumentations and in a way, we share all of
            them. The proposed functionality might have some value but this has to
            be related to the context of use, more specifically  to the social bounds
            connecting the discussants.   The three deployment contexts implicitly
            point to this aspect but the analysis of the three experiences of usage
            is not convincing enough to bring to an articulated set of outcomes. 
            Actually, the context and applied method is different in the three case:
            so any comparison or more general consideration is difficult to make.  
            We agree with one of the reviewer that the paper  contains  a substantial
            body of work but the way in which the proposed technology has been
            validated weakens its overall value. 
            
          %h3 
            Associate Chairs Additional Comments

          %p
            We would suggest to reframe the presentation of the three experiences in
            a more critical and less emphatic way (e.g., according to the suggestions
            of the review #2 and #3 ) and to add to the conclusion  a discussion
            about the obtained results and a more precise plan of how to overcome
            their limitations in future experiments.


      .additional
        .prompt
          .item.fl
            Rebuttal
          %a.hidden.fr
            show
          .clear

        .full.hide

          %p
            Thanks for the thoughtful reviews! It is now clear that we did not effectively articulate the intent of our empirical studies, which led to ambiguity about how they should be reviewed.
          %p
            The goal of the paper is not to objectively prove Reflect works. Rather it is to inspire system designers to confront the challenging problem of web discussion by shifting their attention to listening. Reflect helps make this concrete with a novel design. Our deployments illustrate listening behavior in real communities to inform future designs.
          %p
            The first question a designer of a listening interface must ask is: “if I deploy this interface, will anyone use it to listen?” We will clarify in the paper that our core hypothesis is “interface design can increase evidence of listening in discussions.” We primarily answered this question through the Slashdot deployment, a community one might not expect to engage in active listening given its reputation for trenchant discourse. In the trial, Slashdotters added 1.1 bullets per comment, half of which were neutral restatements. Moreover, community moderation made the listening affordance even more visible. (We later address R3’s concerns about code reliability).
          %p
            Now that we have greater confidence that Reflect will be used to restate points, future research can study in greater depth how Reflect (and other listening interfaces) affect discussions online. But it was important to first establish that real people will use it to listen in real discussions on their own accord.
          %p
            Improving web discussion is a challenging problem. No one CHI paper will solve it, and we need to be able to communicate about creative new approaches. Requiring objective, generalizable assessment of hypothesized affordances, in the same paper that contributes the new social system itself as well as a new theoretical framework, will prevent the community from tackling hard problems.
          %p
            Note: R3 erroneously states that Reflect has already been published. It has only been discussed at non-archival venues: CSCW Horizons & a doctoral consortium abstract.
          %p

          %h3
            RELIABILITY
          %p
            We reported alpha=.72 for our primary code (if a bullet restated the comment). R3 rightly corrects us for calling this “high reliability”, but then states that claims based on data from codes with reliability <.8 are unsound and must be dismissed. .8 is not a hard number; code reliability is to be sought relative to the precision needed for the conclusions to be drawn [1]. See [2] for an example of alphas <.8 being used in high impact political communication research and [1] for a survey of best practices. Even the Wikipedia article R3 refers to states that alphas > .67 are used to draw exploratory conclusions. We only answer broadly whether Reflect is e.g. never, rarely, often or always used to restate. For the other codes, we did not base any analysis on them. We will clarify that we had >90% agreement on all codes, and that low reliability was symptomatic of the rarity of the phenomena (e.g. replying), which is itself useful information. Furthermore, we have refined our coding scheme and re-coded most of the data, with alpha=.89 for the primary code. While the figures have changed slightly (e.g. we find 55.3% of bullets to be neutral restatements, as opposed to 47.4%), our conclusions are the same. We will update the paper accordingly.
          %p
            In context two, we did a small-N interview study, not a focus group. These are valuable because they aid designer intuition about how such designs may be perceived in practice, even if they do not objectively prove a result. Our interviewees include key leaders in the initiatives; their hands-on observations about the affordances and limitations of Reflect draw on years of relevant experience.
          %p
            Finally, we included results from the Mturk experiment because we found it problematic to exclude data showing negative or no results. Given reviewer concern about the preliminary state of analysis, as well as variation in methodology across contexts, we propose to eliminate the A/B aspect of the Mturk section and instead report on the same measures for the Reflect-enabled condition as we did for Slashdot.
          %h3
            GENERALIZABILITY
          %p
            We will more clearly describe the scenarios in which we believe an attention to listening would be most appropriate (R2+AC).
          %p
            To support this framing and compare contexts, we propose to code a sample of bullets from contexts 2 & 3. However, we cannot articulate generalizable outcomes without overclaiming. There are innumerable contexts in which discussion can take place. Even within the same context, variation in the personality of participants can lead to different outcomes. The goal of the paper is to communicate a new direction, not to prove generalizable results.
          %p
            [1] Artstein & Poesio. Inter-coder Agreement for Computational Linguistics. Computational Linguistics, 34(4), 2008.
          %p
            [2] Price, Cappella, & Nir. "Does Disagreement Contribute to More Deliberative Opinion." Political Communication, 19(1), 2002. 


    .clear
